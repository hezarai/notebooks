{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/hezarai/notebooks/blob/main/hezar/00_quick_start.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start\n",
    "We built Hezar to bring all the best works in AI developed by the Persian community together! So that they're more reachable and usable. Besides there's no need to reinvent the wheel for every AI usecase. In this notebook we show you the basics in a few cells of code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first, let's install hezar. In this notebook we'll go through different aspects of Hezar, so it's better to install the full version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install hezar[all]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have everything ready, let's start with loading a ready-to-use model from the Hub!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "There's a bunch of ready to use trained models for different tasks on the Hub!\n",
    "\n",
    "**ðŸ¤—Hugging Face Hub Page**: [https://huggingface.co/hezarai](https://huggingface.co/hezarai)\n",
    "\n",
    "Let's walk you through some examples!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification (sentiment analysis, categorization, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hezar.models import Model\n",
    "\n",
    "example = [\"Ù‡Ø²Ø§Ø±ØŒ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒØ§ÛŒ Ú©Ø§Ù…Ù„ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡ Ú©Ø§Ø±Ú¯ÛŒØ±ÛŒ Ø¢Ø³Ø§Ù† Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ\"]\n",
    "model = Model.load(\"hezarai/bert-fa-sentiment-dksf\")\n",
    "outputs = model.predict(example)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Labeling (POS, NER, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hezar.models import Model\n",
    "\n",
    "pos_model = Model.load(\"hezarai/bert-fa-pos-lscp-500k\")  # Part-of-speech\n",
    "ner_model = Model.load(\"hezarai/bert-fa-ner-arman\")  # Named entity recognition\n",
    "inputs = [\"Ø´Ø±Ú©Øª Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù‡Ø²Ø§Ø±\"]\n",
    "pos_outputs = pos_model.predict(inputs)\n",
    "ner_outputs = ner_model.predict(inputs)\n",
    "print(f\"POS: {pos_outputs}\")\n",
    "print(f\"NER: {ner_outputs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Modeling (Mask Filling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hezar.models import Model\n",
    "\n",
    "roberta_mlm = Model.load(\"hezarai/roberta-fa-mlm\")\n",
    "inputs = [\"Ø³Ù„Ø§Ù… Ø¨Ú†Ù‡ Ù‡Ø§ Ø­Ø§Ù„ØªÙˆÙ† <mask>\"]\n",
    "outputs = roberta_mlm.predict(inputs, top_k=1)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speech Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hezar.models import Model\n",
    "\n",
    "whisper = Model.load(\"hezarai/whisper-small-fa\")\n",
    "transcripts = whisper.predict(\"examples/assets/speech_example.mp3\")\n",
    "print(transcripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image to Text (OCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hezar.models import Model\n",
    "# OCR with TrOCR\n",
    "model = Model.load(\"hezarai/trocr-base-fa-v2\")\n",
    "texts = model.predict([\"examples/assets/ocr_example.jpg\"])\n",
    "print(f\"TrOCR Output: {texts}\")\n",
    "\n",
    "# OCR with CRNN\n",
    "model = Model.load(\"hezarai/crnn-fa-printed-96-long\")\n",
    "texts = model.predict(\"examples/assets/ocr_example.jpg\")\n",
    "print(f\"CRNN Output: {texts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image to Text (License Plate Recognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hezar.models import Model\n",
    "\n",
    "model = Model.load(\"hezarai/crnn-fa-64x256-license-plate-recognition\")\n",
    "plate_text = model.predict(\"assets/license_plate_ocr_example.jpg\")\n",
    "print(plate_text)  # Persian text of mixed numbers and characters might not show correctly in the console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image to Text (Image Captioning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hezar.models import Model\n",
    "\n",
    "model = Model.load(\"hezarai/vit-roberta-fa-image-captioning-flickr30k\")\n",
    "texts = model.predict(\"examples/assets/image_captioning_example.jpg\")\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hezar has support for word embeddings (word2vec, fasttext, etc) too. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hezar.embeddings import Embedding\n",
    "\n",
    "fasttext = Embedding.load(\"hezarai/fasttext-fa-300\")\n",
    "most_similar = fasttext.most_similar(\"Ù‡Ø²Ø§Ø±\")\n",
    "print(most_similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec (Skipgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hezar.embeddings import Embedding\n",
    "\n",
    "word2vec = Embedding.load(\"hezarai/word2vec-skipgram-fa-wikipedia\")\n",
    "most_similar = word2vec.most_similar(\"Ù‡Ø²Ø§Ø±\")\n",
    "print(most_similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec (CBOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hezar.embeddings import Embedding\n",
    "\n",
    "word2vec = Embedding.load(\"hezarai/word2vec-cbow-fa-wikipedia\")\n",
    "most_similar = word2vec.most_similar(\"Ù‡Ø²Ø§Ø±\")\n",
    "print(most_similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hezar is also home to ready-to-use datasets. All of our datasets are hosted on the ðŸ¤—Hub. The cool thing about Hezar's datasets is that\n",
    "besides being able to load all of them using regular `load_dataset` function in ðŸ¤—Datasets, you can also load a dataset from Hub into a ready to use \n",
    "Hezar Dataset class which is a PyTorch compatible Dataset wrapper that can be directly fed to a data loader, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load using Hugging Face datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "sentiment_dataset = load_dataset(\"hezarai/sentiment-dksf\")\n",
    "lscp_dataset = load_dataset(\"hezarai/lscp-pos-500k\")\n",
    "xlsum_dataset = load_dataset(\"hezarai/xlsum-fa\")\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load using Hezar Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hezar.data import Dataset\n",
    "\n",
    "sentiment_dataset = Dataset.load(\"hezarai/sentiment-dksf\")  # A TextClassificationDataset instance\n",
    "lscp_dataset = Dataset.load(\"hezarai/lscp-pos-500k\")  # A SequenceLabelingDataset instance\n",
    "xlsum_dataset = Dataset.load(\"hezarai/xlsum-fa\")  # A TextSummarizationDataset instance\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between using Hezar vs Hugging Face datasets is the output class. In Hezar when you load\n",
    "a dataset using the `Dataset` class, it automatically finds the proper class for that dataset and creates a\n",
    "PyTorch `Dataset` instance so that it can be easily passed to a PyTorch `DataLoader` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from hezar.data.datasets import Dataset\n",
    "\n",
    "dataset = Dataset.load(\n",
    "    \"hezarai/lscp-pos-500k\",\n",
    "    tokenizer_path=\"hezarai/distilbert-base-fa\",  # tokenizer_path is necessary for data collator\n",
    ")\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=dataset.data_collator)\n",
    "itr = iter(loader)\n",
    "print(next(itr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But when loading using Hugging Face datasets, the output is an HF Dataset instance.\n",
    "\n",
    "So in a nutshell, any Hezar dataset can be loaded using HF datasets but not vise-versa!\n",
    "(Because Hezar looks out for a `dataset_config.yaml` file in any dataset repo so non-Hezar datasets cannot be\n",
    "loaded using Hezar `Dataset` class.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hezar also makes it super easy to train or fine-tune models on different datasets on the Hub using the `Trainer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hezar.models import BertSequenceLabeling, BertSequenceLabelingConfig\n",
    "from hezar.data import Dataset\n",
    "from hezar.trainer import Trainer, TrainerConfig\n",
    "from hezar.preprocessors import Preprocessor\n",
    "\n",
    "base_model_path = \"hezarai/bert-base-fa\"\n",
    "dataset_path = \"hezarai/lscp-pos-500k\"\n",
    "\n",
    "train_dataset = Dataset.load(dataset_path, split=\"train\", tokenizer_path=base_model_path)\n",
    "eval_dataset = Dataset.load(dataset_path, split=\"test\", tokenizer_path=base_model_path)\n",
    "\n",
    "model = BertSequenceLabeling(BertSequenceLabelingConfig(id2label=train_dataset.config.id2label))\n",
    "preprocessor = Preprocessor.load(base_model_path)\n",
    "\n",
    "train_config = TrainerConfig(\n",
    "    output_dir=\"bert-fa-pos-lscp-500k\",\n",
    "    task=\"sequence_labeling\",\n",
    "    device=\"cuda\",\n",
    "    init_weights_from=base_model_path,\n",
    "    batch_size=8,\n",
    "    num_epochs=5,\n",
    "    metrics=[\"seqeval\"],\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    config=train_config,\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=train_dataset.data_collator,\n",
    "    preprocessor=preprocessor,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "trainer.push_to_hub(\"bert-fa-pos-lscp-500k\")  # push model, config, preprocessor, trainer files and configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use custom models and datasets with the Trainer by some minor tweaks in your model and dataset classes. Refer to the in-depth guides to find out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
