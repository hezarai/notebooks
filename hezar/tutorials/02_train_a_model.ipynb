{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/hezarai/notebooks/blob/main/hezar/02_train_a_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hezar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a model in Hezar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we're gonna demonstrate a training walkthrough. Training a model in Hezar is pretty much like any other library or even simpler! As mentioned before, any model in Hezar is also a PyTorch module. So training a model is actually training a PyTorch model with some more cool features! Lets dive in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aryan/Applications/miniconda3/envs/main/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from hezar.models import BertTextClassification, BertTextClassificationConfig\n",
    "from hezar.data import Dataset\n",
    "from hezar.trainer import Trainer, TrainerConfig\n",
    "from hezar.preprocessors import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"hezarai/sentiment-dksf\"  # dataset path on the Hub\n",
    "BASE_MODEL_PATH = \"hezarai/bert-base-fa\"  # used as model backbone weights and tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets load a dataset from Hub. For this example we use the Digikala/SnappFood comments datasets which is used for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.load(DATASET_PATH, split=\"train\", tokenizer_path=BASE_MODEL_PATH)\n",
    "eval_dataset = Dataset.load(DATASET_PATH, split=\"test\", tokenizer_path=BASE_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a model for this task and build the model as you would normally do in Hezar (See [models overview](01_models_overview.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertTextClassification(BertTextClassificationConfig(id2label=train_dataset.config.id2label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need the tokenizer for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Preprocessor.load(BASE_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hezar comes with a built-in `Trainer` so that model training is as easy and straightforward as possible. As you might have guessed, in order to use a Trainer we first need to setup the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = TrainerConfig(\n",
    "    output_dir=\"bert-sentiment-fa\",\n",
    "    task=\"text_classification\",\n",
    "    device=\"cuda\",\n",
    "    init_weights_from=BASE_MODEL_PATH,\n",
    "    batch_size=8,\n",
    "    num_epochs=5,\n",
    "    checkpoints_dir=\"checkpoints/\",\n",
    "    metrics=[\"accuracy\", \"f1\"],\n",
    "    num_dataloader_workers=0,\n",
    "    seed=42,\n",
    "    optimizer=\"adam\",\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler=\"reduce_lr_on_plateau\",\n",
    "    save_freq=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that our model is a BERT model with random weights, but we want to finetune it for a simple task. So we need to load the pretrained language model weights. To do this, simply provide the `init_weights_from` parameter which takes a Hub ID to a model and loads the weights to our model. (Missing classification head is automatically ignored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our config, lets build the Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hezar (WARNING): Partially loading the weights as the model architecture and the given state dict are incompatible! \n",
      "Ignore this warning in case you plan on fine-tuning this model\n",
      "Incompatible keys: []\n",
      "Missing keys: ['classifier.weight', 'classifier.bias']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    config=train_config,\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=train_dataset.data_collator,\n",
    "    preprocessor=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aaaannnddd lets train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m******************** Training Info ********************\u001b[0m\n",
      "\n",
      "  \u001b[1mOutput Directory\u001b[0m: `\u001b[3mbert-sentiment-fa\u001b[0m`\n",
      "  \u001b[1mTask\u001b[0m: `\u001b[3mtext_classification\u001b[0m`\n",
      "  \u001b[1mModel\u001b[0m: `\u001b[3mBertTextClassification\u001b[0m`\n",
      "  \u001b[1mInit Weights\u001b[0m: `\u001b[3mhezarai/bert-base-fa\u001b[0m`\n",
      "  \u001b[1mDevice(s)\u001b[0m: `\u001b[3mcuda\u001b[0m`\n",
      "  \u001b[1mTraining Dataset\u001b[0m: `\u001b[3mTextClassificationDataset(path=hezarai/sentiment-dksf['train'], size=28602)\u001b[0m`\n",
      "  \u001b[1mEvaluation Dataset\u001b[0m: `\u001b[3mTextClassificationDataset(path=hezarai/sentiment-dksf['test'], size=2315)\u001b[0m`\n",
      "  \u001b[1mOptimizer\u001b[0m: `\u001b[3madam\u001b[0m`\n",
      "  \u001b[1mInitial Learning Rate\u001b[0m: `\u001b[3m2e-05\u001b[0m`\n",
      "  \u001b[1mLearning Rate Decay\u001b[0m: `\u001b[3m0.0\u001b[0m`\n",
      "  \u001b[1mEpochs\u001b[0m: `\u001b[3m5\u001b[0m`\n",
      "  \u001b[1mBatch Size\u001b[0m: `\u001b[3m8\u001b[0m`\n",
      "  \u001b[1mNumber of Parameters\u001b[0m: `\u001b[3m118299651\u001b[0m`\n",
      "  \u001b[1mNumber of Trainable Parameters\u001b[0m: `\u001b[3m118299651\u001b[0m`\n",
      "  \u001b[1mMixed Precision\u001b[0m: `\u001b[3mFull (fp32)\u001b[0m`\n",
      "  \u001b[1mMetrics\u001b[0m: `\u001b[3m['accuracy', 'f1']\u001b[0m`\n",
      "  \u001b[1mCheckpoints Path\u001b[0m: `\u001b[3mbert-sentiment-fa/checkpoints/\u001b[0m`\n",
      "  \u001b[1mLogs Path\u001b[0m: `\u001b[3mbert-sentiment-fa/logs\u001b[0m`\n",
      "\n",
      "\u001b[1m*******************************************************\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5      100%|######################################################################| 3575/3575 [06:43<00:00,  8.87batch/s, loss=0.611]\n",
      "Evaluating...   100%|######################################################################| 289/289 [00:07<00:00, 37.84batch/s, accuracy=0.839, f1=0.744, loss=0.411]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2/5      100%|######################################################################| 3575/3575 [06:39<00:00,  8.95batch/s, loss=0.464]\n",
      "Evaluating...   100%|######################################################################| 289/289 [00:07<00:00, 39.09batch/s, accuracy=0.761, f1=0.657, loss=0.566]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3/5      100%|######################################################################| 3575/3575 [06:42<00:00,  8.89batch/s, loss=0.335]\n",
      "Evaluating...   100%|######################################################################| 289/289 [00:07<00:00, 37.68batch/s, accuracy=0.871, f1=0.806, loss=0.371]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4/5      100%|######################################################################| 3575/3575 [06:45<00:00,  8.81batch/s, loss=0.216]\n",
      "Evaluating...   100%|######################################################################| 289/289 [00:07<00:00, 38.70batch/s, accuracy=0.875, f1=0.809, loss=0.388]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5/5      100%|######################################################################| 3575/3575 [06:43<00:00,  8.86batch/s, loss=0.148]\n",
      "Evaluating...   100%|######################################################################| 289/289 [00:07<00:00, 37.69batch/s, accuracy=0.848, f1=0.77, loss=0.58]  \n",
      "Hezar (INFO): Training done!\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we trained the model for 5 epochs. As you can see, everything is verbosed during the process. After each epoch all metrics and weights are logged and saved. Tensorboard logs are saved to a folder called `runs` (you can change this default) and you can inspect it as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir runs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the weights are saved to `checkpoints` (you can change this default)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push to Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can push our model along with some training specific configs to the Hub! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub(\"hezarai/bert-fa-sentiment-digikala-snappfood\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
